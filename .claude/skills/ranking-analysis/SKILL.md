---
name: ranking-analysis
description: ë„¤ì´ë²„ ì‡¼í•‘ ìƒí’ˆ ìˆœìœ„ ë¶„ì„ ë° ì¶”ì  ì „ë¬¸ ìŠ¤í‚¬. ìˆœìœ„ ë°ì´í„° ìˆ˜ì§‘, í†µê³„ ë¶„ì„, íŠ¸ë Œë“œ íŒŒì•…, A/B í…ŒìŠ¤íŠ¸ ë¹„êµê°€ í•„ìš”í•  ë•Œ ì‚¬ìš©. í‚¤ì›Œë“œ: ìˆœìœ„, ë­í‚¹, í†µê³„, ë¶„ì„, íŠ¸ë Œë“œ, A/B í…ŒìŠ¤íŠ¸, ìˆœìœ„ ì¶”ì 
allowed-tools: Read, Write, Bash(python:*), Grep, Glob
---

# Ranking Analysis Specialist

ë„¤ì´ë²„ ì‡¼í•‘ ìƒí’ˆ ìˆœìœ„ ë¶„ì„ ë° ì¶”ì  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í†µê³„ì  ë°©ë²•ë¡ ì„ í™œìš©í•˜ì—¬ ìˆœìœ„ ë³€ë™ì„ ë¶„ì„í•˜ê³  ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ì „ë¬¸ ë¶„ì•¼

### 1. ìˆœìœ„ ë°ì´í„° ìˆ˜ì§‘ ë° ê´€ë¦¬
- ë„¤ì´ë²„ ì‡¼í•‘ ìˆœìœ„ í¬ë¡¤ë§
- ìˆœìœ„ ë°ì´í„° ì •ê·œí™” ë° ê²€ì¦
- ì‹œê³„ì—´ ë°ì´í„° ê´€ë¦¬
- ë°ì´í„° ë¬´ê²°ì„± í™•ì¸

### 2. í†µê³„ ë¶„ì„
- ìˆœìœ„ ë³€ë™ í†µê³„ (í‰ê· , ì¤‘ì•™ê°’, í‘œì¤€í¸ì°¨)
- A/B í…ŒìŠ¤íŠ¸ ë¹„êµ (t-test, ì¹´ì´ì œê³± ê²€ì •)
- ì´ìƒì¹˜ íƒì§€ ë° ì œê±°
- ì‹ ë¢°êµ¬ê°„ ê³„ì‚°

### 3. íŠ¸ë Œë“œ ë¶„ì„
- ì‹œê°„ëŒ€ë³„ ìˆœìœ„ ë³€ë™ íŒ¨í„´
- ìš”ì¼ë³„/ì‹œê°„ëŒ€ë³„ íš¨ê³¼ì„± ë¶„ì„
- ê³„ì ˆì„± ë° ì£¼ê¸°ì„± íƒì§€
- ì¶”ì„¸ì„  ë° ì˜ˆì¸¡ ëª¨ë¸

### 4. ì‹œê°í™” ë° ë³´ê³ 
- ëŒ€ì‹œë³´ë“œ ìƒì„±
- ê·¸ë˜í”„ ë° ì°¨íŠ¸ ì œì‘
- ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ ë° ìš”ì•½
- ë³´ê³ ì„œ ìë™ ìƒì„±

## ìˆœìœ„ ë°ì´í„° êµ¬ì¡°

### ê¸°ë³¸ ë°ì´í„° í¬ë§·

```python
{
    "product_id": "12345678",
    "test_case_id": 1,
    "iteration": 1,
    "timestamp": "2025-01-01 12:00:00",
    "keyword": "ë¬´ì„  ì´ì–´í°",
    "rank": {
        "page": 3,          # í˜ì´ì§€ ë²ˆí˜¸ (1ë¶€í„° ì‹œì‘)
        "position": 12,     # í˜ì´ì§€ ë‚´ ìœ„ì¹˜ (1~40)
        "absolute_rank": 92 # ì ˆëŒ€ ìˆœìœ„: (page-1)*40 + position
    },
    "metadata": {
        "price": 49900,
        "review_count": 1234,
        "rating": 4.5,
        "delivery": "ë¬´ë£Œë°°ì†¡",
        "brand": "ë¸Œëœë“œëª…"
    },
    "test_info": {
        "test_type": "A",  # A: ë„¤ì´ë²„ ê²€ìƒ‰, B: ì‡¼í•‘ ì§ì ‘
        "ip_address": "123.456.789.0",
        "user_agent": "Mozilla/5.0...",
        "device_id": "RF8M12345XY"
    }
}
```

### ìˆœìœ„ ë³€ë™ ë°ì´í„° í¬ë§·

```python
{
    "product_id": "12345678",
    "test_case_id": 1,
    "iteration": 1,
    "before_rank": {
        "timestamp": "2025-01-01 12:00:00",
        "absolute_rank": 92,
        "page": 3,
        "position": 12
    },
    "after_rank": {
        "timestamp": "2025-01-01 12:30:00",
        "absolute_rank": 68,
        "page": 2,
        "position": 28
    },
    "rank_change": +24,  # ì–‘ìˆ˜: ìƒìŠ¹, ìŒìˆ˜: í•˜ë½
    "page_moved": True,  # í˜ì´ì§€ ì´ë™ ë°œìƒ ì—¬ë¶€
    "success": True,     # ìˆœìœ„ ìƒìŠ¹ ì„±ê³µ ì—¬ë¶€
    "test_duration": 1800  # í…ŒìŠ¤íŠ¸ ì†Œìš” ì‹œê°„ (ì´ˆ)
}
```

## ë¶„ì„ ë©”íŠ¸ë¦­

### 1. ê¸°ë³¸ í†µê³„

```python
import numpy as np
import pandas as pd

def calculate_basic_stats(rank_changes):
    """
    ìˆœìœ„ ë³€ë™ ê¸°ë³¸ í†µê³„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    stats = {
        # ì¤‘ì‹¬ ê²½í–¥ì„±
        'mean': np.mean(rank_changes),
        'median': np.median(rank_changes),
        'mode': pd.Series(rank_changes).mode()[0],

        # ì‚°í¬ë„
        'std': np.std(rank_changes),
        'variance': np.var(rank_changes),
        'range': np.max(rank_changes) - np.min(rank_changes),
        'iqr': np.percentile(rank_changes, 75) - np.percentile(rank_changes, 25),

        # ë¶„í¬ íŠ¹ì„±
        'min': np.min(rank_changes),
        'max': np.max(rank_changes),
        'q1': np.percentile(rank_changes, 25),
        'q3': np.percentile(rank_changes, 75),

        # ì„±ê³µë¥ 
        'success_rate': sum(1 for x in rank_changes if x > 0) / len(rank_changes),
        'total_count': len(rank_changes)
    }

    return stats

# ì‚¬ìš© ì˜ˆì‹œ
rank_changes = [+12, -3, +18, +5, +8, +22, -1, +15, +9, +11]
stats = calculate_basic_stats(rank_changes)

print(f"í‰ê·  ìˆœìœ„ ë³€ë™: {stats['mean']:.2f}ìœ„")
print(f"ì¤‘ì•™ê°’: {stats['median']:.2f}ìœ„")
print(f"í‘œì¤€í¸ì°¨: {stats['std']:.2f}")
print(f"ì„±ê³µë¥ : {stats['success_rate']*100:.1f}%")
```

### 2. ì•ˆì •ì„± ì§€ìˆ˜

ìˆœìœ„ ë³€ë™ì˜ ì¼ê´€ì„±ì„ ì¸¡ì •í•©ë‹ˆë‹¤. ì•ˆì •ì„±ì´ ë†’ì„ìˆ˜ë¡ ì˜ˆì¸¡ ê°€ëŠ¥í•œ íš¨ê³¼ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.

```python
def calculate_stability_index(rank_changes):
    """
    ì•ˆì •ì„± ì§€ìˆ˜ = í‰ê·  / í‘œì¤€í¸ì°¨
    ë†’ì„ìˆ˜ë¡ ì•ˆì •ì  (ë‚®ì€ ë³€ë™ì„±ìœ¼ë¡œ ì¼ê´€ëœ íš¨ê³¼)
    """
    mean = np.mean(rank_changes)
    std = np.std(rank_changes)

    if std == 0:
        return float('inf')  # ì™„ë²½í•œ ì•ˆì •ì„±

    stability = mean / std
    return stability

# ì˜ˆì‹œ
case_a_changes = [+15, +16, +14, +17, +15]  # ì•ˆì •ì 
case_b_changes = [+25, -5, +10, +30, -10]   # ë¶ˆì•ˆì •

print(f"ì¼€ì´ìŠ¤ A ì•ˆì •ì„±: {calculate_stability_index(case_a_changes):.2f}")
print(f"ì¼€ì´ìŠ¤ B ì•ˆì •ì„±: {calculate_stability_index(case_b_changes):.2f}")
```

### 3. íš¨ê³¼ í¬ê¸° (Effect Size)

```python
def calculate_cohen_d(group1, group2):
    """
    Cohen's d: ë‘ ê·¸ë£¹ ê°„ íš¨ê³¼ í¬ê¸° ì¸¡ì •
    0.2 = ì‘ìŒ, 0.5 = ì¤‘ê°„, 0.8 = í¼
    """
    mean1, mean2 = np.mean(group1), np.mean(group2)
    std1, std2 = np.std(group1), np.std(group2)
    n1, n2 = len(group1), len(group2)

    # Pooled standard deviation
    pooled_std = np.sqrt(((n1-1)*std1**2 + (n2-1)*std2**2) / (n1+n2-2))

    cohen_d = (mean1 - mean2) / pooled_std
    return cohen_d

# ì˜ˆì‹œ: ì¼€ì´ìŠ¤ A vs B
case_a = [+15, +12, +18, +14, +16]
case_b = [+8, +6, +10, +7, +9]

effect_size = calculate_cohen_d(case_a, case_b)
print(f"Effect Size (Cohen's d): {effect_size:.2f}")

if abs(effect_size) < 0.2:
    print("íš¨ê³¼ í¬ê¸°: ì‘ìŒ")
elif abs(effect_size) < 0.5:
    print("íš¨ê³¼ í¬ê¸°: ì¤‘ê°„")
else:
    print("íš¨ê³¼ í¬ê¸°: í¼")
```

## í†µê³„ì  ìœ ì˜ì„± ê²€ì¦

### 1. t-test (ë‘ ê·¸ë£¹ ë¹„êµ)

```python
from scipy import stats

def compare_two_cases(case_a, case_b, alpha=0.05):
    """
    ë‘ ì¼€ì´ìŠ¤ì˜ ìˆœìœ„ ë³€ë™ì„ ë¹„êµí•˜ì—¬ í†µê³„ì  ìœ ì˜ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.
    """
    # ë…ë¦½í‘œë³¸ t-ê²€ì •
    t_stat, p_value = stats.ttest_ind(case_a, case_b)

    result = {
        't_statistic': t_stat,
        'p_value': p_value,
        'is_significant': p_value < alpha,
        'mean_a': np.mean(case_a),
        'mean_b': np.mean(case_b),
        'mean_difference': np.mean(case_a) - np.mean(case_b)
    }

    # í•´ì„
    if result['is_significant']:
        if result['mean_difference'] > 0:
            result['interpretation'] = f"ì¼€ì´ìŠ¤ Aê°€ ì¼€ì´ìŠ¤ Bë³´ë‹¤ í‰ê·  {result['mean_difference']:.2f}ìœ„ ë” íš¨ê³¼ì  (p < {alpha})"
        else:
            result['interpretation'] = f"ì¼€ì´ìŠ¤ Bê°€ ì¼€ì´ìŠ¤ Aë³´ë‹¤ í‰ê·  {abs(result['mean_difference']):.2f}ìœ„ ë” íš¨ê³¼ì  (p < {alpha})"
    else:
        result['interpretation'] = f"ë‘ ì¼€ì´ìŠ¤ ê°„ í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì°¨ì´ ì—†ìŒ (p = {p_value:.3f})"

    return result

# ì‚¬ìš© ì˜ˆì‹œ
case_a_data = [+15, +12, +18, +14, +16, +13, +17, +15, +14, +16]
case_b_data = [+8, +6, +10, +7, +9, +8, +10, +7, +8, +9]

result = compare_two_cases(case_a_data, case_b_data)
print(result['interpretation'])
```

### 2. ìŒì²´ t-test (Before/After ë¹„êµ)

```python
def compare_before_after(before_ranks, after_ranks, alpha=0.05):
    """
    ë™ì¼ ìƒí’ˆì˜ í…ŒìŠ¤íŠ¸ ì „í›„ ìˆœìœ„ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.
    """
    # ìˆœìœ„ ë³€ë™ ê³„ì‚° (Before - After, ìŒìˆ˜ë©´ ìˆœìœ„ ìƒìŠ¹)
    rank_changes = np.array(before_ranks) - np.array(after_ranks)

    # ë‹¨ì¼í‘œë³¸ t-ê²€ì • (ê·€ë¬´ê°€ì„¤: í‰ê·  ë³€ë™ = 0)
    t_stat, p_value = stats.ttest_1samp(rank_changes, 0)

    result = {
        't_statistic': t_stat,
        'p_value': p_value,
        'is_significant': p_value < alpha,
        'mean_change': np.mean(rank_changes),
        'median_change': np.median(rank_changes)
    }

    # í•´ì„
    if result['is_significant']:
        if result['mean_change'] > 0:
            result['interpretation'] = f"í…ŒìŠ¤íŠ¸ë¡œ ì¸í•´ í‰ê·  {result['mean_change']:.2f}ìœ„ ìƒìŠ¹ (í†µê³„ì ìœ¼ë¡œ ìœ ì˜, p < {alpha})"
        else:
            result['interpretation'] = f"í…ŒìŠ¤íŠ¸ë¡œ ì¸í•´ í‰ê·  {abs(result['mean_change']):.2f}ìœ„ í•˜ë½ (í†µê³„ì ìœ¼ë¡œ ìœ ì˜, p < {alpha})"
    else:
        result['interpretation'] = f"í…ŒìŠ¤íŠ¸ ì „í›„ ìœ ì˜í•œ ìˆœìœ„ ë³€ë™ ì—†ìŒ (p = {p_value:.3f})"

    return result
```

### 3. ì¹´ì´ì œê³± ê²€ì • (ë²”ì£¼í˜• ë°ì´í„°)

```python
def compare_success_rates(case_a_success, case_a_total, case_b_success, case_b_total):
    """
    ë‘ ì¼€ì´ìŠ¤ì˜ ì„±ê³µë¥ ì„ ë¹„êµí•©ë‹ˆë‹¤.
    """
    # ë¶„í• í‘œ ìƒì„±
    observed = [
        [case_a_success, case_a_total - case_a_success],  # ì¼€ì´ìŠ¤ A: ì„±ê³µ, ì‹¤íŒ¨
        [case_b_success, case_b_total - case_b_success]   # ì¼€ì´ìŠ¤ B: ì„±ê³µ, ì‹¤íŒ¨
    ]

    chi2, p_value, dof, expected = stats.chi2_contingency(observed)

    case_a_rate = case_a_success / case_a_total
    case_b_rate = case_b_success / case_b_total

    result = {
        'chi2': chi2,
        'p_value': p_value,
        'case_a_success_rate': case_a_rate,
        'case_b_success_rate': case_b_rate,
        'rate_difference': case_a_rate - case_b_rate
    }

    # í•´ì„
    if p_value < 0.05:
        result['interpretation'] = f"ë‘ ì¼€ì´ìŠ¤ì˜ ì„±ê³µë¥ ì´ í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•˜ê²Œ ë‹¤ë¦„ (p < 0.05)"
    else:
        result['interpretation'] = f"ë‘ ì¼€ì´ìŠ¤ì˜ ì„±ê³µë¥ ì´ í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì°¨ì´ ì—†ìŒ (p = {p_value:.3f})"

    return result

# ì˜ˆì‹œ
# ì¼€ì´ìŠ¤ A: 100íšŒ ì¤‘ 89íšŒ ì„±ê³µ
# ì¼€ì´ìŠ¤ B: 100íšŒ ì¤‘ 76íšŒ ì„±ê³µ
result = compare_success_rates(89, 100, 76, 100)
print(f"ì¼€ì´ìŠ¤ A ì„±ê³µë¥ : {result['case_a_success_rate']*100:.1f}%")
print(f"ì¼€ì´ìŠ¤ B ì„±ê³µë¥ : {result['case_b_success_rate']*100:.1f}%")
print(result['interpretation'])
```

## ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ë¶„ì„

### 1. ì‹œê°„ëŒ€ë³„ í‰ê·  ìˆœìœ„ ë³€ë™

```python
import pandas as pd

def analyze_by_time_of_day(data):
    """
    ì‹œê°„ëŒ€ë³„ ìˆœìœ„ ë³€ë™ íŒ¨í„´ì„ ë¶„ì„í•©ë‹ˆë‹¤.
    data: [{'timestamp': '2025-01-01 14:30:00', 'rank_change': +12}, ...]
    """
    df = pd.DataFrame(data)
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['hour'] = df['timestamp'].dt.hour

    # ì‹œê°„ëŒ€ ë¶„ë¥˜
    def classify_time(hour):
        if 6 <= hour < 12:
            return 'ì˜¤ì „'
        elif 12 <= hour < 18:
            return 'ì˜¤í›„'
        elif 18 <= hour < 24:
            return 'ì €ë…'
        else:
            return 'ì‹¬ì•¼'

    df['time_period'] = df['hour'].apply(classify_time)

    # ì‹œê°„ëŒ€ë³„ í†µê³„
    time_stats = df.groupby('time_period')['rank_change'].agg([
        ('í‰ê· ', 'mean'),
        ('ì¤‘ì•™ê°’', 'median'),
        ('í‘œì¤€í¸ì°¨', 'std'),
        ('í…ŒìŠ¤íŠ¸ íšŸìˆ˜', 'count'),
        ('ì„±ê³µë¥ ', lambda x: sum(x > 0) / len(x))
    ]).round(2)

    return time_stats

# ì‚¬ìš© ì˜ˆì‹œ
data = [
    {'timestamp': '2025-01-01 08:00:00', 'rank_change': +14},
    {'timestamp': '2025-01-01 14:00:00', 'rank_change': +11},
    {'timestamp': '2025-01-01 20:00:00', 'rank_change': +9},
    # ... ë” ë§ì€ ë°ì´í„°
]

time_stats = analyze_by_time_of_day(data)
print(time_stats)
```

### 2. ìš”ì¼ë³„ íŒ¨í„´ ë¶„ì„

```python
def analyze_by_day_of_week(data):
    """
    ìš”ì¼ë³„ ìˆœìœ„ ë³€ë™ íŒ¨í„´ì„ ë¶„ì„í•©ë‹ˆë‹¤.
    """
    df = pd.DataFrame(data)
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['day_of_week'] = df['timestamp'].dt.day_name()

    # ìš”ì¼ë³„ í†µê³„
    day_stats = df.groupby('day_of_week')['rank_change'].agg([
        ('í‰ê· ', 'mean'),
        ('ì¤‘ì•™ê°’', 'median'),
        ('í‘œì¤€í¸ì°¨', 'std'),
        ('í…ŒìŠ¤íŠ¸ íšŸìˆ˜', 'count'),
        ('ì„±ê³µë¥ ', lambda x: sum(x > 0) / len(x))
    ]).round(2)

    # ì£¼ì¤‘ vs ì£¼ë§ ë¹„êµ
    weekday_mask = df['timestamp'].dt.dayofweek < 5  # ì›”~ê¸ˆ
    weekday_mean = df[weekday_mask]['rank_change'].mean()
    weekend_mean = df[~weekday_mask]['rank_change'].mean()

    print(f"ì£¼ì¤‘ í‰ê· : {weekday_mean:.2f}ìœ„")
    print(f"ì£¼ë§ í‰ê· : {weekend_mean:.2f}ìœ„")
    print(f"ì£¼ì¤‘-ì£¼ë§ ì°¨ì´: {weekday_mean - weekend_mean:.2f}ìœ„")

    return day_stats
```

### 3. íˆíŠ¸ë§µ ë°ì´í„° ìƒì„±

```python
def create_heatmap_data(data):
    """
    ìš”ì¼ Ã— ì‹œê°„ëŒ€ íˆíŠ¸ë§µ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    df = pd.DataFrame(data)
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['day_of_week'] = df['timestamp'].dt.day_name()
    df['hour'] = df['timestamp'].dt.hour

    # í”¼ë²— í…Œì´ë¸” ìƒì„±
    heatmap = df.pivot_table(
        values='rank_change',
        index='day_of_week',
        columns='hour',
        aggfunc='mean'
    ).round(2)

    return heatmap
```

## ì´ìƒì¹˜ íƒì§€

### 1. IQR ë°©ë²•

```python
def detect_outliers_iqr(rank_changes):
    """
    IQR(Interquartile Range) ë°©ë²•ìœ¼ë¡œ ì´ìƒì¹˜ë¥¼ íƒì§€í•©ë‹ˆë‹¤.
    """
    q1 = np.percentile(rank_changes, 25)
    q3 = np.percentile(rank_changes, 75)
    iqr = q3 - q1

    # ì´ìƒì¹˜ ê²½ê³„
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr

    outliers = [x for x in rank_changes if x < lower_bound or x > upper_bound]
    normal = [x for x in rank_changes if lower_bound <= x <= upper_bound]

    result = {
        'outliers': outliers,
        'normal': normal,
        'outlier_count': len(outliers),
        'outlier_rate': len(outliers) / len(rank_changes),
        'bounds': (lower_bound, upper_bound)
    }

    return result

# ì‚¬ìš© ì˜ˆì‹œ
rank_changes = [+12, +15, +8, +68, +14, -25, +11, +13, +10, +16]
outliers = detect_outliers_iqr(rank_changes)

print(f"ì´ìƒì¹˜: {outliers['outliers']}")
print(f"ì´ìƒì¹˜ ë¹„ìœ¨: {outliers['outlier_rate']*100:.1f}%")
```

### 2. Z-score ë°©ë²•

```python
def detect_outliers_zscore(rank_changes, threshold=3):
    """
    Z-score ë°©ë²•ìœ¼ë¡œ ì´ìƒì¹˜ë¥¼ íƒì§€í•©ë‹ˆë‹¤.
    threshold: ì¼ë°˜ì ìœ¼ë¡œ 3 (í‘œì¤€í¸ì°¨ 3ë°°)
    """
    mean = np.mean(rank_changes)
    std = np.std(rank_changes)

    z_scores = [(x - mean) / std for x in rank_changes]
    outliers = [x for x, z in zip(rank_changes, z_scores) if abs(z) > threshold]

    return outliers
```

## ì‹œê°í™”

### 1. ìˆœìœ„ ë³€ë™ ì¶”ì´ ê·¸ë˜í”„

```python
import matplotlib.pyplot as plt

def plot_rank_trend(data):
    """
    ì‹œê°„ì— ë”°ë¥¸ ìˆœìœ„ ë³€ë™ ì¶”ì´ë¥¼ ê·¸ë˜í”„ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
    """
    df = pd.DataFrame(data)
    df['timestamp'] = pd.to_datetime(df['timestamp'])

    plt.figure(figsize=(12, 6))
    plt.plot(df['timestamp'], df['absolute_rank'], marker='o', linewidth=2)
    plt.xlabel('ì‹œê°„')
    plt.ylabel('ì ˆëŒ€ ìˆœìœ„')
    plt.title('ìƒí’ˆ ìˆœìœ„ ë³€ë™ ì¶”ì´')
    plt.gca().invert_yaxis()  # ìˆœìœ„ëŠ” ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig('rank_trend.png', dpi=300)
    plt.close()
```

### 2. ë°•ìŠ¤í”Œë¡¯ (ì¼€ì´ìŠ¤ ë¹„êµ)

```python
def plot_boxplot_comparison(case_a, case_b):
    """
    ë‘ ì¼€ì´ìŠ¤ì˜ ìˆœìœ„ ë³€ë™ì„ ë°•ìŠ¤í”Œë¡¯ìœ¼ë¡œ ë¹„êµí•©ë‹ˆë‹¤.
    """
    data = [case_a, case_b]
    labels = ['ì¼€ì´ìŠ¤ A\n(ë„¤ì´ë²„ ê²€ìƒ‰)', 'ì¼€ì´ìŠ¤ B\n(ì‡¼í•‘ ì§ì ‘)']

    plt.figure(figsize=(8, 6))
    plt.boxplot(data, labels=labels)
    plt.ylabel('ìˆœìœ„ ë³€ë™ (ìœ„)')
    plt.title('ì¼€ì´ìŠ¤ë³„ ìˆœìœ„ ë³€ë™ ë¶„í¬ ë¹„êµ')
    plt.grid(True, alpha=0.3, axis='y')
    plt.axhline(y=0, color='r', linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.savefig('boxplot_comparison.png', dpi=300)
    plt.close()
```

### 3. íˆíŠ¸ë§µ (ì‹œê°„ëŒ€ë³„ íš¨ê³¼)

```python
import seaborn as sns

def plot_heatmap(heatmap_data):
    """
    ìš”ì¼ Ã— ì‹œê°„ëŒ€ íˆíŠ¸ë§µì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    plt.figure(figsize=(14, 6))
    sns.heatmap(heatmap_data, annot=True, fmt='.1f', cmap='RdYlGn', center=0)
    plt.xlabel('ì‹œê°„ (Hour)')
    plt.ylabel('ìš”ì¼')
    plt.title('ìš”ì¼ Ã— ì‹œê°„ëŒ€ë³„ í‰ê·  ìˆœìœ„ ë³€ë™')
    plt.tight_layout()
    plt.savefig('heatmap.png', dpi=300)
    plt.close()
```

## ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ

### ìë™ ì¸ì‚¬ì´íŠ¸ ìƒì„±

```python
def generate_insights(stats, case_a, case_b, time_stats):
    """
    ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì£¼ìš” ì¸ì‚¬ì´íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    insights = []

    # 1. í‰ê·  ìˆœìœ„ ë³€ë™
    if stats['mean'] > 10:
        insights.append(f"âœ… í‰ê·  {stats['mean']:.1f}ìœ„ ìƒìŠ¹ìœ¼ë¡œ ëª©í‘œ(+10ìœ„) ë‹¬ì„±")
    else:
        insights.append(f"âš ï¸  í‰ê·  {stats['mean']:.1f}ìœ„ ìƒìŠ¹ìœ¼ë¡œ ëª©í‘œ(+10ìœ„) ë¯¸ë‹¬ì„±")

    # 2. ì„±ê³µë¥ 
    if stats['success_rate'] > 0.85:
        insights.append(f"âœ… ë†’ì€ ì„±ê³µë¥ ({stats['success_rate']*100:.1f}%) ìœ ì§€")
    else:
        insights.append(f"âš ï¸  ì„±ê³µë¥ ({stats['success_rate']*100:.1f}%) ê°œì„  í•„ìš”")

    # 3. ì¼€ì´ìŠ¤ ë¹„êµ
    case_a_mean = np.mean(case_a)
    case_b_mean = np.mean(case_b)
    if case_a_mean > case_b_mean:
        insights.append(f"ğŸ“Š ì¼€ì´ìŠ¤ Aê°€ ì¼€ì´ìŠ¤ Bë³´ë‹¤ í‰ê·  {case_a_mean - case_b_mean:.1f}ìœ„ ë” íš¨ê³¼ì ")
    else:
        insights.append(f"ğŸ“Š ì¼€ì´ìŠ¤ Bê°€ ì¼€ì´ìŠ¤ Aë³´ë‹¤ í‰ê·  {case_b_mean - case_a_mean:.1f}ìœ„ ë” íš¨ê³¼ì ")

    # 4. ìµœì  ì‹œê°„ëŒ€
    best_time = time_stats['í‰ê· '].idxmax()
    best_avg = time_stats['í‰ê· '].max()
    insights.append(f"â° {best_time} ì‹œê°„ëŒ€ê°€ ê°€ì¥ íš¨ê³¼ì  (í‰ê·  +{best_avg:.1f}ìœ„)")

    # 5. ì•ˆì •ì„±
    stability = calculate_stability_index(case_a + case_b)
    if stability > 1.0:
        insights.append(f"âœ… ë†’ì€ ì•ˆì •ì„± ì§€ìˆ˜({stability:.2f})ë¡œ ì˜ˆì¸¡ ê°€ëŠ¥í•œ íš¨ê³¼")
    else:
        insights.append(f"âš ï¸  ë‚®ì€ ì•ˆì •ì„± ì§€ìˆ˜({stability:.2f})ë¡œ ë³€ë™í­ í¼")

    return insights
```

## ì‚¬ìš© ê°€ì´ë“œ

### ì–¸ì œ ì´ Skillì´ ë°œë™ë˜ëŠ”ê°€?

ì‚¬ìš©ìê°€ ë‹¤ìŒê³¼ ê°™ì€ ìš”ì²­ì„ í•˜ë©´ ì´ Skillì´ ìë™ìœ¼ë¡œ ë°œë™ë©ë‹ˆë‹¤:
- "ìˆœìœ„ ë³€ë™ ë¶„ì„í•´ì¤˜"
- "ì¼€ì´ìŠ¤ Aë‘ B ì¤‘ ì–´ë–¤ ê²Œ ë” íš¨ê³¼ì ì´ì•¼?"
- "ìµœê·¼ 1ì£¼ì¼ ìˆœìœ„ ì¶”ì´ë¥¼ ë³´ì—¬ì¤˜"
- "í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì°¨ì´ê°€ ìˆì–´?"
- "ì–´ë–¤ ì‹œê°„ëŒ€ì— í…ŒìŠ¤íŠ¸í•˜ëŠ” ê²Œ ì¢‹ì•„?"

### ì°¸ì¡° ë¬¸ì„œ

ìƒì„¸í•œ ë©”íŠ¸ë¦­ ê°€ì´ë“œëŠ” @metrics-guide.md ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.

## ê¶Œì¥ ë¶„ì„ í”„ë¡œì„¸ìŠ¤

1. **ë°ì´í„° ìˆ˜ì§‘ ë° ì •ì œ**: ì´ìƒì¹˜ ì œê±°, ê²°ì¸¡ì¹˜ ì²˜ë¦¬
2. **ê¸°ë³¸ í†µê³„ ê³„ì‚°**: í‰ê· , ì¤‘ì•™ê°’, í‘œì¤€í¸ì°¨
3. **í†µê³„ì  ê²€ì¦**: t-test, ì¹´ì´ì œê³± ê²€ì •
4. **íŒ¨í„´ ë¶„ì„**: ì‹œê°„ëŒ€ë³„, ìš”ì¼ë³„ ë¶„ì„
5. **ì‹œê°í™”**: ê·¸ë˜í”„, ì°¨íŠ¸ ìƒì„±
6. **ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ**: ì£¼ìš” ë°œê²¬ì‚¬í•­ ìš”ì•½
7. **ê¶Œì¥ ì‚¬í•­ ì œì‹œ**: ë‹¤ìŒ ì•¡ì…˜ ê°€ì´ë“œ
